Sangkil Moon, Paul K. Bergey, & Dawn Iacobucci
Dynamic Effects Among Movie
Ratings, Movie Revenues, andViewer
Satisfaction

There are numerous industries in which experts offer opinions about the quality of products and brands. For example, movie critics make suggestions about a soon-to-be released movie’s artistic and entertainment value, BusinessWeek hosts Robert Parker’s column recommending wines, Consumer Reports has long compared brands across numerous product categories, and so forth. In addition, consumers are increasingly posting online evaluations of products and brands—for example, they review books on Amazon.com, movies on Netflix.com, video games on Gamespot.com, or restaurants on Citysearch.com. Consumers find judgments from both professional critics and amateur communities to be helpful, in part because the sheer number of new products and the frequency of their launches (e.g., weekly releases for movies) can be over whelming for consumers in the choice process. In addition, many such products appear wholly unique, so a comparison of the movies Terminator Salvation and X-Men Origins: Wolverine or a comparison of the wines Argentinian Malbec and Italian Prosecco is difficult; thus, both critics’ and other ordinary consumers’ evaluations assist in decision making.
Professional critics commonly provide reviews and ratings; this information signals unobservable product quality and helps consumers make good choices (Boulding and Kirmani 1993; Kirmani and Rao 2000). Although amateur consumers can obtain useful information from critics, they are sometimes at odds with critics because of some funda mental differences between the two groups in terms of experiences and preferences (Chakravarty, Liu, and Mazumdar 2008; Holbrook 1999; Wanderer 1970). Therefore, consumers often seek like-minded amateurs’ opinions in various ways. 
The recent development and proliferation of online consumer review forums, in which consumers share opinions on products, has had an enormous impact on the dynamics of word of mouth (WOM) by effectively connecting consumers (Chen and Xie 2008; Eliashberg, Elberse, and Leenders 2006; Godes and Mayzlin 2004; Godes et al. 2005; Mayzlin 2006; Trusov, Bucklin, and Pauwels 2009). These online forums lower the product information search costs, which motivates consumers to seek such review information (Stigler 1961). After all, consumer communities’ collective opinions can have as much influence on other consumers’ choices as professional critics’ opinions. In addition to these two influence groups, consumers make choices in accordance with their own judgments based on past experiences in the given product category, which can be contrary to opinions from either professional critics or amateur com munities. In this sense, consumers are active information processors rather than passive information receivers (Bettman 1970). 
To provide a comprehensive evaluation of how product reviews and ratings influence consumers’ choices and satisfaction arising from their experiential consumption, we consider the opinions from these multiple sources. Such external and internal information sources are particularly important in movie choice because viewers constantly face the problem of choosing satisfying movies among many new and existing ones. Indeed, the development of the Internet has engendered movie rental service Web sites (e.g., Net flix, Blockbuster), on which members can access a wealth of movie review information with minimal effort. Members can also post their own opinions with ease. In such an environment, the influence of online member communities’ general opinions on movie choice is maximized (Chevalier and Mayzlin 2006; Liu 2006). Although the contributions of this research are intended to encompass multiple industries, we focus on the movie industry, in part because of its sizable financial contribution to the general economy ($10 billion in 2008 U.S. box office ticket revenues according to www.the-numbers.com/market). 
This research attempts to highlight the relationships between product ratings and product financial performance—more specifically, various sources of movie ratings and movie performances (i.e., movie revenues and viewer satisfaction)—while considering various movie quality characteristics (e.g., movie costs, original versus sequel). To accomplish this objective, we conduct an empirical analysis at two levels: the (aggregate) movie level and the (individual) viewer level. First, we focus on the movie-level analysis to examine the two-way dynamic influences between movie ratings and movie revenues. In this analysis, we view movie revenues as the collective results of individual viewers’ choices. In doing so, we focus on collective critics’ and amateur communities’ ratings for each movie but not on individual viewers’ ratings. Second, to supplement this aggregate view, we examine how individual viewers’ movie consumption influences their postconsumption evaluations. To do so, we conduct a viewer-level analysis, in which we test the influence of both the focal viewer’s viewing and rating history and the movie community’s collective opinions on the focal viewer’s new movie rating, while controlling for movie quality (i.e., movie characteristics). 
This two-level analysis approach enables us to examine the relationships between movie ratings and movie performances from complementary angles and to provide important managerial insights. Importantly, the (macro) movie level analysis captures moviegoers’ collective choices in the movie industry, whereas the (micro) viewer-level analysis taps into individual consumers’ postconsumption experiences. We develop and test five hypotheses based on this two-way classification. 
From a managerial perspective, on the basis of some key empirical findings, this research suggests that movie marketers should persistently promote movies that garner high ratings to sustain movie revenues and should cautiously consider sequels despite their originals’ commercial success. For movie rental firms, this study provides insights into ways to recommend movies on the basis of the focal member’s rating history, the member community’s overall movie rating patterns, and the movie’s characteristics. Net flix maintains that approximately 60% of its members select movies according to movie recommendations tailored to their tastes. In addition, our results should be applicable to other consumption situations in which consumers continually face new products (e.g., new books, new music albums) and determine the expected value of the new products according to their own experiences, like-minded amateur communities’ general opinions, and critics’ professional reviews. 
In the following section, we discuss the theoretical background and develop hypotheses pertaining to the relationships between movie ratings and performances. Our empirical analyses test the hypotheses using both movie level data and viewer-level data. Finally, we discuss the managerial implications of our findings
Hypotheses Development 
Movie-Level Effects: Movie Ratings and Revenues (H1–H3)
Prior research has developed movie revenue evaluation models in various contexts, focusing particularly on theater revenues (Ainslie, Drèze, and Zufryden 2005; Eliashberg et al. 2000; Jedidi, Krider, and Weinberg 1998) rather than on video revenues (Prosser 2002). Some research has also examined the impact of critics’ ratings and reviews on theater revenues. Specifically, in an empirical examination, Eliashberg and Shugan (1997) find that critical reviews are correlated with late and cumulative box office receipts but not with early box office receipts; thus, they conclude that critics are predictors rather than influencers. In contrast, some studies show that critics play a dual role as both predictors and influencers (Basuroy, Chatterjee, and Ravid 2003; Boatwright, Basuroy, and Kamakura 2007). According to Reinstein and Snyder (2005), when movies receive high ratings from either critics or ordinary viewers, revenues increase. Early high ratings can generate positive WOM that can spread to ordinary viewers. 
Less obvious, however, is the hypothesis we test regarding whether strong revenues can subsequently generate more positive reviews during the course of the movie. If we can confirm this reciprocating dynamic interaction between reviews and revenues, we would establish that high ratings effectively sustain high movie revenues and vice versa over the not-so-long life of the movie. Our reasoning is similar to that of Duan, Gu, and Whinston (2008), who indicate that a unique aspect of the WOM effect is the presence of a positive feedback mechanism between WOM and sales. Simi larly, Godes and Mayzlin (2004) theorize that commercially successful television shows can engender more buzz among ordinary viewers. The enhanced buzz is usually positive for commercially successful movies because of the generally positive correlation between movie ratings and movie revenues. In other words, movie viewers talk more about successful movies, which affects revenues and ratings, than unsuccessful movies. Behavioral learning theory characterizes this dynamic process as vicarious learning because consumers learn from the market and the process positively reinforces their satisfaction (Nord and Peter 1980; Rothschild and Gaidis 1981). The favorable enhanced buzz from high-revenue movies contributes to enhanced movie ratings in following weeks because there are more viewers who had positive experiences with the movie. This relationship is strengthened by the mutual confirmation of the online community environment composed of ordinary viewers. 
Movie marketers are known to enhance advertising spending for movies that were commercially successful in preceding weeks, which in turn draws more positive reviews from movie viewers.1 In other words, advertising can also play a role in confirming viewer satisfaction, which translates into higher movie ratings in subsequent weeks (Spreng, MacKenzie, and Olshavsky 1996). Thus, we test the following hypothesis:
H1: High movie revenues enhance subsequent movie ratings. 
This hypothesis implies that early successful revenues from early adopters of new products serve as an information cue for late adopters’ purchases and satisfaction. Word of mouth is known to be a powerful and effective force, assisting the diffusion of consumer packaged goods, durables, and services in the market. Early commercial success is also a proxy of assurance of high quality from early adopters, a segment often acknowledged as experts in the relevant product category; as such, early sales figures lend credibility to the product launch, which in turn enhances the product’s success. 
The movie literature takes somewhat of a signaling theory’s perspective, in that a consumer who witnesses the early commercial success of a movie can infer that the movie has qualities that make it popular and might also infer that the movie has artistic or creative merit. The literature maintains that the same quality signal from two sources—one from marketers (advertising) and the other from consumer communities (ratings)—can effectively influence consumers’ choices in the marketers’ favor by greatly reducing uncertainties about new products’ unobservable quality (Kirmani 1997; Nelson 1974). 
Some recent research suggests that in the movie industry, certain signals may become less useful in the presence of others; for example, Basuroy, Desai, and Talukdar (2006) find the attenuating role of third-party information sources (e.g., critics’ review consensus and cumulative WOM) on the strength of the adverting signal. Other research argues that advertising and ratings indeed function synergistically, enhancing revenues when well-known movies (those with large budgets and heavy advertising) receive positive WOM (high movie ratings) (Basuroy, Chatterjee, and Ravid 2003; Liu 2006). To contribute to this line of inquiry, we also test an interactive hypothesis from analogous sources. We predict an interactive effect that advertising spending upgrades revenues further when ratings are more positive. Our theorizing suggests that if movie ratings are positive, potential consumers are more likely to respond to the advertising, thus enhancing the effect of advertising on revenues.
Note that both factors must be in play; neither is sufficient on its own.2 That is, positive ratings alone cannot effectively increase revenues, because not enough potential viewers know about the movie. In addition, highly advertised movies cannot generate enough revenue without favorable ratings from ordinary moviegoers, because negative WOM spreads more quickly for these types of movies than for others. Yet we anticipate that neither piece of information is sufficient, because the effect is interactive and synergistic. Our theorizing may be consistent with signaling theory, if both sets of signals are calibrated to be equally effective. Realistically, we acknowledge that one set of signals may seem to be more diagnostic as a cue than another set (Lynch 2006; Milgrom and Roberts 1986). Thus, we predict an interactive effect, but without specifying that one contributing signal attenuates another: 
H2: Positive ratings enhance the effectiveness of advertising spending to raise movie revenues. 
Movie sequels build on the original movies’ commercial success (Basuroy and Chatterjee 2008). That is, moviegoers tend to view the high quality of the original movie as a sig nal of the quality of a sequel because they tend to associate various products of the same brand with product quality (Erdem 1998). With generous production budgets and heavy advertising based on the original movie’s brand power, a sequel usually achieves box office success, even if it does not meet the box office levels attained by the parent movie (Basuroy and Chatterjee 2008; Ravid 1999). 
Although sequels can make money, they are often rated less favorably than original movies. That is, the original movie’s success leads to high expectations for the sequel, which are often difficult to meet, thus leading to less satis faction (Anderson 1973; Oliver 2009). Viewers may be less satisfied and less impressed as a result of satiation on expe riential attributes arising from a sequel’s lack of novelty and surprise, which results in lower ratings by moviegoers. Fig uratively speaking, when the punch line is known, the humor is less effective. Distancing a movie somewhat from the expectations of the original pays off; Sood and Drèze (2006) find that dissimilar sequels were rated higher than similar ones and that sequels with descriptive titles (e.g., Pirates of the Caribbean: Dead Man’s Chest) were rated higher than those with numbered titles (e.g., Spider-Man 2). Low ratings on a sequel tend to spread in the movie popula tion, thus limiting its revenues in following weeks, which does not bode well for a sequel in the long run. Such low ratings of sequels may partially explain why subsequent sequels are rarely made. 
Therefore, we hypothesize that the effects of both higher revenues and lower ratings of sequels are likely realized predominately in the early weeks after release because sequels tend to stimulate their loyal consumer base quickly. That is, viewers who already liked an original movie will tend to see its sequel earlier than a new, unknown, original movie. Accordingly, these sequel movie effects are also likely to dissipate quickly and therefore would not be as strongly pronounced a few weeks after release. Thus, we predict the following: 
H3: Sequels reap higher revenues but lower ratings than originals, predominately in the early weeks after release.
General Viewer-Level Effects: Own Past Ratings and Community Opinions 
Movie revenues are one indicator of the results of consumers’ collective choices. Individual viewers’ ratings are another significant measure, effectively summarizing consumer satisfaction in the movie industry. Higher ratings may lead viewers to choose other movies that share preferred characteristics (e.g., sequel, same actor), and the reasons for satisfaction can be spread to online consumer communities through text reviews. Viewer preferences can develop into a stable and established preference for viewers, such as favorite genres or favorite stars. This is particularly important for online movie rental firms (e.g., Netflix) because members have unlimited access to other members’ ratings and reviews. For these firms, members’ increased satisfaction enhances their loyalty to the company. Movie rental firms invest tremendous time and effort into developing an effective movie recommendation system based on (1) the individual member’s rating history, (2) the member community’s overall rating patterns of the movie of interest, and (3) the movie’s characteristics. The firms can then use the system for customer relationship management by using these information sources. Beyond such a recommendation system, it is equally important to understand how such information sources are associated with members’ ratings on new movies for insights into how the new ratings are determined. 
To understand the individual member’s ratings, we examine six factors: (1) number of movies rated, (2) average rating, (3) rating standard deviation, (4) percentage of same genre, (5) genre-specific average rating, and (6) most recent rating. Next, we describe the anticipated effect of each factor. 
First, the member’s number of movies rated indicates the frequency of movie viewing, which is used to identify segments traditionally referred to as “light” and “heavy” segments. Overall movie-viewing frequency indicates general liking toward movies, with heavy viewers liking movies more. Therefore, we expect a positive association between number of movies rated and new movie rating. Second, the member’s average rating shows how strict or lenient he or she is when rating movies on average. Lenient raters tend to rate new movies higher than strict raters, and accordingly, we expect a positive association between average rating and new movie rating. Third, the member’s rating standard deviation represents the variability of the member’s ratings across different movies and reflects his or her risk tolerance in choosing movies. In other words, a wide rating variability may indicate that a member’s choices have been experimental and risky; in such a case, the member can end up with more disappointing movies than members with a narrow rating variability. Thus, we expect a negative association between rating standard deviation and new movie rating.
Fourth, the member’s percentage of each movie genre in his or her rating history measures how often the member sees movies of the same genre. For example, thriller junkies view most movies in the genre because they like the genre the most of all the movie genres. This internal disposition tends to lead them to rate thriller movies high. Thus, we expect a positive association between percentage of same genre and new movie rating. Fifth, the member’s historical average movie rating for the same genre also measures his or her general preference toward movies of the same genre. Logically, we expect that a rating for a new movie of the same genre is positively associated with a member’s general preference toward the same genre. Sixth, we theorize that a recent satisfactory movie experience raises the aspiration level, whereas a recent disappointing experience lowers the aspiration level (Anderson 1973). According to prospect theory (Kahneman and Tversky 1979), the aspiration level set by the recent movie experience should function as a reference point, yielding a negative association with most recent rating and next rating. 
Next, we turn to an understanding of the second factor group of the movie recommendation system, the member community’s overall rating patterns; to do so, we examine five factors: (1) number of ratings, (2) average rating, (3) rating standard deviation, (4) percentage of highest rating, and (5) percentage of lowest rating. These factors echo community opinions and are comparable to online WOM effects. First, the community’s accumulated number of ratings of a movie indicates how many members have already seen the movie. Because more interested members view the movie before less interested members, we expect that the accumulated number of ratings is negatively correlated with the new rating. Second, we expect that the community’s historical average rating of the movie of interest is positively correlated with the new rating because members tend to rate the same movie similarly—that is, most viewers rate good movies high and bad movies low. Third, the community’s historical rating standard deviation of the movie of interest measures the degree of evaluation disagreement toward the same movie. More diversely discussed movies (i.e., those with high rating standard deviations) can attract more viewers by raising their curiosity than less discussed movies, especially in the ubiquitous and freely accessible online movie community environment. However, negative reviews of a movie tend to disappoint viewers who are attracted by increased discussion because these viewers are likely to have high expectations from positive comments about the movie. Thus, we expect a negative association between rating standard deviation and a new movie rating. Regarding the fourth and fifth points, the community’s percentage of the highest and lowest ratings of the movie of interest indicates two opposite extreme ratings (e.g., 5 and 1 on a five point scale, respectively) and is a strong indicator of new ratings beyond the community’s simple average rating. Thus, the highest rating is positively correlated with a new movie rating, and the lowest rating is negatively correlated with a new movie rating. 
These two data perspectives, along with movie characteristics, converge to lend a better understanding of how both consumers’ own consumption experiences and community opinions influence consumers’ postconsumption evaluations in the movie category. Recently, marketing scholars have emphasized “connected” consumers in the Internet era, in which people can easily exchange consumption information (Chen and Xie 2008; Chevalier and Mayzlin 2006). However, consumers still value and use their own experiences (i.e., internal information) in decision making, in addition to community opinions (i.e., external information). In general, consumers are likely to seek various types of information sources to reduce uncertainty as the perceived risk associated with a purchase increases (Murray 1991; West and Broniarczyk 1998). In the movie industry, the uniqueness of each movie makes movie choice challenging, along with the financial and transaction costs (e.g., ticket price, travel to the theater). 
In-Depth Viewer-Level Effects: Rating Pattern Developments with Experiences (H4–H5) 
In the previous subsection, we focused on illuminating two groups of factors that influence amateur viewers’ new ratings—viewers’ own past ratings and movie communities’ opinions. Here, we also consider how amateur viewers’ ratings develop as these viewers acquire more movie consumption experiences (Alba and Hutchinson 1987). Therefore, in this subsection, we present two in-depth hypotheses on individual viewers’ rating pattern developments: one hypothesis on viewers’ rating changes over time (H4) and one hypothesis on how viewers’ movie consumption experiences are associated with their genre preferences (H5). 
We examine how amateur viewers’ ratings can change over time. We test and verify that members with more ratings experiences rate movies lower, similar to critics’ ratings, which are generally lower than amateurs’ ratings because of the critical nature of their reviews.4 By watching more movies, members develop a reliable, large reference base and, accordingly, should be able to analyze movies similarly to professional critics. Alba and Hutchinson (1987) indicate that increased familiarity (i.e., the number of product-related experiences) results in increased consumer expertise (i.e., the ability to perform product-related tasks successfully). Furthermore, we expect members to choose preferred movies first and then a set of movies that do not include their best choices. 
At the same time, members’ ratings become less variable with experiences because their consumption experiences become stabilized over time. Specifically, on the one hand, it becomes more difficult to satisfy them, and thus they give high ratings less often. On the other hand, their improved expertise and accumulated experience enable them to avoid movies that are unsuitable to their tastes, and thus they give low ratings less often. Therefore, amateurs’ movie ratings become stabilized in the form of less variability with consumption experiences. 
H4: Amateur viewers’ movie-viewing experiences generate less favorable ratings with less variability. 
In the long run, amateur viewers’ ratings should stabilize at a certain level because there will not be any more substantial learning experience in critiquing movies. Therefore, this hypothesis is primarily focused on amateur viewers who are acquiring relative new movie consumption experiences as opposed to seasoned and experienced amateur viewers. 
Next, given the association between movie preferences and ratings with genre (Liu 2006), such as children being fans of animation movies, we expect that viewers give their favorite genres (more precisely, members’ frequently viewed genres) high ratings because they are internally predisposed to like that category of movies (upward “preferred genre effect”). In contrast, as we predict in H4, as viewers choose more movies beyond their best choices in their nonfavorite genres, they may rate those movies lower without having the preferred genre effect as in their favorite genres (downward “viewed set effect”). That is, as viewers choose more movies, they settle for less attractive movies because they have exhausted their top choices in certain genres. Thus, the relationship between genre proportion (i.e., the percentage of movies seen in the genre compared with all movies seen for that individual viewer) and average genre rating may be nonmonotonic because of these two conflicting effects and warrants further investigation. 
Specifically, we expect “genre loyalists” with a high range of genre proportions to generate high ratings approximately proportional to their genre proportion because of their strong internal inclination toward their favorite genres. In most cases, their strong inclination toward their frequently viewed genres prevents them from choosing from other, less favored genres. That is, genre loyalists are strongly predisposed to specific aspects of their favorite genres. For example, thriller movie loyalists enjoy how the story unfolds and entertains their own anticipated scenarios. Thus, such strong preferences for their favorite genres lead the loyalists to rate most movies in their preferred genres favorably (upward effect by preferred genres). In contrast, this effect should be weak or nonexistent for viewers who balance multiple genres, and accordingly, they should exhibit a downward effect by viewed set. Finally, we expect a low range of genre proportions to result in a medium range of ratings due to viewers’ choosing only the most recognizable movies in a genre that is only a little known to them (e.g., through intensive advertising exposure, friends’ strong recommendation). However, their lack of strong inclination toward a particular genre leads them to have only a moderate level of satisfaction, despite the movie’s strengths.
Importantly, we hypothesize that this U-shaped relationship is pronounced only for heavy viewers, who gain enough consumption experiences through enhanced analysis and elaboration abilities to process product information (Alba and Hutchinson 1987). We do not expect the relationship to be strong for light viewers (i.e., novices), because their experiences do not allow them to fully develop either the upward effect (preferred genre effect) or the downward effect (viewed set effect). 
H5: There is a U-shaped relationship between experienced viewers’ genre proportion (i.e., genre preference) and their genre rating. 
Empirical Analysis 
We divide our empirical analyses into two parts according to the different nature of the available data. First, we provide the empirical results for H1–H3, using movie-level data from various sources, including Rotten Tomatoes for professional critics’ ratings and Yahoo Movies for amateurs’ ratings. These data do not include information on individual critics or individual amateur viewers. Second, with individual members’ data mainly from Netflix, we run a regression analyzing individual viewers’ movie ratings to test H4 and H5.
Movie-Level Data 
The data contain specific information regarding 246 movies that cover six major genre categories: thriller, romance, action, drama, comedy, and animation. The movies were released during the May 2003–October 2005 period in theaters and on video. Table 1 provides a summary on these data, such as movie characteristics, costs, and revenues. 
We gathered the ratings information on the 246 movies from two popular movie Web sites: Rotten Tomatoes and Yahoo Movies. Both sites allow members to post movie ratings but differ in terms of membership conditions. The Rotten Tomatoes site comprises accredited movie critics exclusively. Accordingly, these members are active in either select movie critic societies/associations or print publications. They are regarded as professional movie critics. In contrast, the Yahoo Movies site is open to the public and, for the most part, comprises amateur movie lovers (see Table 1). 
Movie-Level Analysis: Movie Ratings and Revenues (H1–H3) 
In the weekly regression summarized in Table 2, high weekly theater revenues induced more favorable ratings from amateurs in the following week in six of the seven weeks tested, in support of H1. Week 1 was the only exception (with theater revenues in Week 0 [opening week] and movie ratings in Week 1), which suggests that viewers in the opening week (Week 0) tended to have mixed evaluations about the movie, perhaps because they saw the movie for different reasons. For example, heavy advertising from movie marketers or insufficient and inconsistent information from like-minded moviegoers can make satisfactory choices difficult. During the opening week, however, enough people view the movie and spread their reviews and ratings in both online and offline domains. Thus, in general, late viewers make better-informed choices, which is empirically supported by more positive ratings in the following weeks. In the process, commercially successful movies can generate more satisfactory viewers who then rate the movies higher. That is, these viewers choose the movies because of previous ratings and reviews. 
We tested H2 regarding the interaction effects of movie ratings (from either critics or amateurs) with ad spending on box office revenues using weekly data. Table 3 provides the significant variables at the .05 level (for the initial independent variables used in each regression, see the Appendix). In the weekly analysis, we used the previous week’s ad spending measures (i.e., weekly ad spending, weekly theater ad proportion, and weekly movie ad proportion) and the accumulated movie ratings (i.e., critics’ and amateurs’ ratings) up to the previous week to measure their effects on the following week’s theater revenues. We used the accumulated ratings because moviegoers can review all the past ratings to determine which movie to see. 
In this weekly analysis, we confirmed that the interaction effects of ratings and spending (critics’ ratings × ad spending and amateurs’ ratings × ad spending) were significant in Week 2–Week 7, whereas the main effects of ratings were not. This implies that movie revenues cannot be maximized without the combination of ratings and ad spending in these weeks; thus, H2 is empirically supported for the later weeks. The nonsignificant main effects of both ratings (critics’ and amateurs’ ratings) indicate that ratings alone cannot increase movie revenues without enhanced buzz created through advertising spending. In contrast, we observe a mixture of positive and negative main effects of the movie cost variables (i.e., weekly ad spending, weekly theater ad proportion, and weekly movie ad proportion). Although we expect that movie costs are positively correlated with movie revenues, significantly negative movie cost effects imply that some advertising money was excessively wasted beyond its proper use, based on its effective combination with favorable ratings (measured by both interaction terms). In other words, negative movie cost effects occurred after the regression was accounted for by both significantly positive interaction term effects. Notably, our empirical analysis shows that movie marketers tend to allocate more advertising dollars to movies that collect high revenues in preceding weeks. It demonstrates that in many cases, marketers used advertising money inefficiently because they did not consider both revenues and ratings when allocating their advertising resources. 
In contrast, in the opening week (Week 0), we found that only one main effect (critics’ ratings) and one interaction effect (amateurs’ ratings × ad spending) were significant. The result of Week 0 implies that critics’ ratings have a significant main effect on theater revenues because critics’ reviews and ratings are intensively published in various out lets shortly before opening week (Week 0). In the same week, amateurs’ ratings showed no significant main effect, probably because there are only a limited number of amateur reviews before a movie’s release. Accordingly, high amateur ratings can only enhance revenues with the help of substantial ad spending in the week. In the following week (Week 1), we observe one significant main effect (amateurs’ ratings) and one significant interaction effect (critics’ ratings × ad spending). In this particular week, amateurs’ ratings create enough information and buzz from early movie goers, and thus the still-new movies do not need the support of heavy advertising to enhance revenues. In contrast, the combination of critics’ ratings and ad spending enhances movie revenues effectively beginning this week. After the first two weeks, because of reduced voluntary attention and buzz among ordinary viewers, only a combination of good ratings and heavy ad spending made a substantial influence on theater revenues. 
Next, we tested H3, which compares sequels and their contemporaneous originals. Table 3 shows that the positive impact of sequels on theater revenues occurred only in the first two weeks after the movie’s release and that the impact was much stronger in the opening week (Week 0) than in the following week (Week 1). Afterward, the impact weakened, probably because the buzz and attention for the sequel dissipated quickly. Our weekly analysis in Table 2 shows a negative impact of a sequel on movie ratings in Weeks 1 and 2 as well. In brief, the empirical results show that sequels can have a positive impact on theater revenues based on the originals’ success, but they leave viewers unimpressed and unsatisfied relative to the original movies. Yet these sequel effects are pronounced only in the early weeks and become subsequently neutralized because the fan base stemming from the original tends to view the sequel early. 
Viewer-Level Data 
We used individual members’ movie rental and rating histories data from the Netflix Prize site for the individual viewer–level analysis. The public data contain more than 100 million ratings of 17,770 movie titles from 480,000 randomly chosen, anonymous Netflix members. We collected the data between October 1998 and December 2005, and they reflect the distribution of all ratings received during the period. The title and release year of each movie are also provided. 
From the Netflix Prize public data, we selected 13,734,151 ratings of the 246 movies used for our previous movie-level analysis and matched this viewer-level data with the movie-level data. The data included 456,476 Net flix members. The ratings selected cover the Jun 2001–December 2005 period. The rating average of the 246 movies was 3.38 on a five-point scale, with an average of 55,830 ratings for a movie. 
General Viewer-Level Analysis: Own Past Ratings and Community Opinions 
We developed a regression model comprised of three groups of factors that we anticipated should influence new movie ratings (dependent variable): (1) individual member– based variables (X), (2) community-based variables (Y), and (3) movie characteristics variables (Z) (see Table 4). We analyzed the impacts of the three groups of factors on new movie ratings to provide comprehensive empirical findings regarding consumer satisfaction with movies. Thus, we used the following linear regression, in which the dependent variable, R, represents member h’s rating for movie mat time t (West and Broniarczyk 1998). Our regression reflects continuously updated temporal information at the given time point to evaluate the new rating R: 
(1)	Rhmt = α + βXhmt + γYmt + δZm + εhmt , 
where the Xhmt variables represent the rating member’s individual viewing experiences and preferences. The X term, which is varied for the three dimensions of member (h), movie (m), and time (t), is composed of the six specific individual member variables based on the focal member’s viewing and rating history. Next, the five Ymt variables measure community opinions, which are comparable to online WOM effects. Unlike X, Y varies in the dimensions of movie (m) and time (t) but does not vary across members (h) as collective group opinions. Finally, Zm includes 11 movie characteristics variables. We use these variables as control variables to measure more accurately how both the X and the Y variables influence the dependent variable; Z does not vary across members (h) or time (t) as fixed movie (m) characteristics. 
To fit this model, we removed members with fewer than 10 ratings over the 246 movies. Then, we randomly selected 1 of every 300 members to make the regression more man ageable. The selected sampling resulted in 43,204 ratings from 1014 members. The average rating across the members was 3.59. The average number of ratings of the members was 42.6 of the 246 movies. 
Table 4 shows the estimation results of the regression model. All the individual member–based variables (X) and community-based variables (Y) showed the expected signs, and 9 of the 11 X and Y variables were significant at the .05 level. Between the two insignificant variables (X3 and X6), the most recent rating (X6) was close to the cutoff level. Despite its expected sign, rating standard deviation (X3) was insignificant, perhaps because wide rating variability elicits both high ratings and low ratings across members. Notably, all the five community-based variables were significant. In particular, the results of percentage of highest rating (Y4) and percentage of lowest rating (Y5) imply that the two extreme ratings are strong indicators of additional ratings from new viewers of the same movies beyond the average rating (Y2). Basuroy, Chatterjee, and Ravid (2003) find that negative reviews hurt performance more than positive reviews help performance (negativity bias). Our research confirms this theory because the absolute estimate value of the percentage of the lowest rating (.02176) is much larger than that of the percentage of the highest rating (.01376). 
Finally, most movie characteristics variables (Z) were also significant. All five genre dummy variables were significant, which indicates their differential impact on ratings. The sequel dummy variable has a negative impact on the new rating, which is consistent with our previous empirical results, in support of H3. The Motion Picture Association of America (MPAA) rating dummy variable has a positive sign, which indicates that R-rated movies are rated higher than non-R-rated movies. Similarly, a longer movie tends to be rated higher. After we account for all the movie-related factors in the regression, production budget has a negative impact on ratings. This result implies that big-budget movie producers tend to spend excessively beyond financially justifiable quality improvements. In association with video release factors, videos released on holidays and those released shortly after their theater release tend to be rated higher. Finally, both video bonus materials and high video ad spending show a positive impact on ratings. In brief, the results of the Z variables add face validity to our overall regression results. 
In-Depth Viewer-Level Analysis: Rating Pattern Developments with Experiences (H4–H5)
Table 5 shows our temporal trend analysis of individual members’ rating changes over time as they view more movies. For this analysis, we sorted the Netflix data used for the regression in Table 4 in the ascending order of the rating time for each member. We applied generalized least squares estimation to correct for the heteroskedasticity (i.e., unequal variances) problem in the data (Goldberger 1991; Griffiths, Hill, and Judge 1993). Table 5, Panel A, shows the relationship between members’ viewing experiences (member rating order) and the rating mean across all members at the overall level and each of all the six genres, which is consistently negative. The negative ratings at the overall level imply that members tend to rate recently viewed movies more strictly as they acquire more consumption experiences. The same pattern was confirmed in five of the six genres at the .10 significance level. Similarly, Table 5, Panel B, shows that members’ratings tend to become less variable with consumption experiences at the overall level. Again, the same pattern is confirmed in five of the six genres at the .10 significance level. In addition, we found that the correlation between the movie rating average and the rating frequency was positive across movies (.557, p ≤ .01) but negative across members (–.064, p ≤ .01). The positive correlation across movies implies that popular movies are rated higher; however, the negative correlation across members indicates that more experiences can make viewers tougher raters, in support of H4 (see also Table 5). 
We tested the hypothesized U-shaped relationship between the member’s genre proportion and genre-specific rating average (H5) and confirmed the curvilinear relationship (Figure 1). We theorized that genre loyalists, who see a majority of movies in one or two genres, rate movies higher because of their strong preference for the favorite genres. Furthermore, perhaps they do a better job at selecting good movies in their favorite genres because of their genre-specific expertise. In contrast, genre switchers, who tend to watch various movies across genres, often rate movies lower because they settle for less satisfactory movies after they exhaust their top choices in chosen genres. This genre related rating pattern becomes more pronounced for heavy viewers (i.e., experts) than for light viewers (i.e., novices). 
Figure 1, Panel B, illustrates our empirical results of the relationship among viewers overall, heavy viewers, and light viewers according to the regression results in Panel A. In the heavy-viewers segment, which shows the most pronounced relationship among the three groups, the genre specific movie rating becomes lower until the genre-specific proportion reaches 23% (for genre switchers). As mentioned previously, we expect that these viewers sample less satisfactory movies in their nonfavorite genres after exhausting their favorite movies in the same genres. After they passed the lowest threshold, the genre rating began to increase quickly and reached the highest possible rating (i.e., five) when the member’s genre proportion hit 77%.
